# AI 모형의 파라미터 크기와 GPU 램의 상관 관계 - 블랙스완 숙제

![img_29.png](..%2Fimages%2Fimg_29.png)

솔직히 GPT4나 Claude Sonnet 3.5랑 대화해서 요약하려다가 걍 관뒀어. 얘네도 너~~~무 장황하게만 설명하고 별 도움이 안 돼서... 왜 그러냐면 얘넨 너무 잘 알아서 그래. 인간이 뭘 오해해서 이걸 헷갈리는 지 모르거든. 영어 원어민이 한국인의 고충을 이해 못 하듯이ㅠ.ㅠ

그래서 걍 내가 다시 썼어. 원래 영어로 썼던 글도 있고.

https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/book/sidebars/model-parameters-and-vram-requirements-can-my-gpu-handle-it/Model-Parameters-And-Vram-Requirements-Can-My-Gpu-Handle-It.md

일단 github  인공지능 온라인 책사이드바 꼼꼼히 읽으시는데... 컴쟁이 백그라운드가 없으면 좀 어려울테니 대두족장 성격에 안 어울리게 좀 친절하게 일단 한글로 설명해드릴게. (형이 오늘 맛난 고기 먹어서 기분이 좀 좋아... 좋잖아🤗)

간단히 이런 의문을 해결하시라는 거야.

8B 파라미터 모형(Llama 3.1 8B 같은 거)이 있는데, 내 노트북에 달린 NVidia GPU가 8GB 짜리고(아니면 애플 실리콘 통합 메모리 8GB 짜리 그런 거), 왜 버벅대냐는 거지. 분명 딱 맞거나 조금 여유가 있는데도 GPU 안 쓰고 CPU로 떠넘기냐는 거지.

내가 쓰는 맥스튜디오도 192GB 짜린데도, 70~80B 모형은 제대로 못 돌려. 언뜻 남아도는 데도.

이게 일단 정밀도 문제거든. 정밀도 안 밝히고 8B라고 하면 의미가 없어. 다음 공식 외워두셔.

Parameters = Weights + Biases

파리미터는 모형의 가중치와 편향을 합해 놓은 거야.

가중치 수가 워낙 많고 편향은 레이어당 하나뿐이라 걍 퉁쳐서 weights = parameters 라고 말하기도 해. Open weights 식으로 하는 말이 그런 거야. 아마 인공지능 쟁이 대부분이 평소엔 weights라는 말을 쓸 거야. 나도 그렇고.  이 바닥 친구들은 이 문맥을 알고 뱉는 소리고 대부분은 모르고 왜곡하는 소리라는 거지.

가중치만으로 설명해도 큰 문제는 없어. 편향보다 절대적으로 많으니까. 실제 bias값은 가중치 보정치 수준이기도 하고.

어쨌든... 저 8B 파라미터라는 건 파라미터의 개수지 용량이 아냐. 이게 흔히 하는 오해야. 8B is not 8GB! 이걸 용량으로 나타내면...

- Full precision(32비트 부동소수점): 8B x 4 Bytes = 32GB
- Half precision(16비트 부동소수점): 8B x 2 Bytes = 16GB
- Quarter precision(8비트 정수): 8B x 1 Byte = 8GB

그러니까 개수와 용량을 혼동하면 서로 다른 단위를 비교하는 거라고.  

간단히 사사오입 떠올리셔. 3.1415926535 이런 거. 여기서 크기를 줄이는 거야. 정보손실 감안해서. 대개 3.14만으로 충분하니까. 이걸 정수까지 끌어올리면 걍 3만 쓰는 거야. 소수점 다 버리고. 

대부분의 경우 3.14면 충분하잖아, 근데 로켓 만들때도 이짓하면 로켓 터진다고. 오차가 점점 심해지니까. 지구에서 떠날때는 오차가 적다가 궤도에 가까워질수록 그 오차가 겁나 커지면서 뻥. 이해하실 거야.

이 파라미터라는 게 다 이런 정밀도가 있어. 거의 무손실이면 4바이트, 32비트 실수를 사용하는 거고 1바이트면 int 정수야. 요즘은 4bit(half byte) 까지 사용해. 궁극적으로 1bit 까지 가려는 친구들도 있어.

그럼 어떤 일이 벌어지냐면, 살짝씩 인공지능이 멍청해지는 거야. 근데 우리보단 겁나 똑똑하니까 그 오차가 심하게 안 느껴지는 거지. 간단히 정밀도를 희생하면서 dumb down 하는 거라고.

응...그러니까 8B 모형이라고 해도 정밀도를 봐야해. HuggingFace에 올라오는 건 대부분 half precision이니까 (특별히 아니라는 말이 없으면) 16GB라고. 그래서 내 GPU 램이 8GB라도 못 돌리는 거야. 모형 파라미터 8B라는 건 바이트 수가 아니라 개수라고. 여기에 바이트를 곱해야 실제 필요한 GPU 램 용량이 나오는 거야.

메탈 GPU가 통합된 애플 실리콘은 쫌 애매해. 이론적으로는 맥스튜디오 울트라 192GB라면 이 192GB 메모리를 모두 GPU로도 쓸 수 있다는 건데, 현실적으론 구라야. 내 경험으로는 (왜 내 경험이라고 하냐면, 공식 자료를 발표 안 하거든 애플이) 약 70~80% 정도가 가용 GPU 램이야. 오버헤드도 고려해야하기 때문에 (모형만 돌릴 게 아니니까, 모형 로더도 감안해야 하고) 70% 정도 잡으면 그나마 안전한 거야.

192 x 0.7 = 134GB 정도. 

근데, 이건 1 byte 모형인 거라고. 결국. 

70B x 2 bytes = 140GB.
80B x 2 bytes = 160GB.

그래서 Llama 70B나 80B를 못 돌리는 거야. 실제로 못 돌려. 올려도 버벅 거려. CPU까지 동원해야 하기 때문에.

이럴 때 쓰는게 모형 압축 기법인 pruning이니 quantization이니 하는 건데, 기본은 이미 설명한 정밀도 희생해서 파라미터 크기를 줄이는 거야. 3.1415926535를 3.14로 줄이는 거랑 같은 맥락. Pruning은 아예 파라미터 수를 줄이는 거라 조금 달라. 목표는 같고.

Lossless 음악이랑 320KB MP3 음악... 뭐 그런 거 떠올리셔도 돼. 객체지향적인 거라 개념은 비슷해. 상속받으라고. 인공지능 모형쪽의 다형성일 뿐이야. Normalization과 compression 개념을 상속받아서 오디오든 모형이든 다형성 적용하는 거라고. 

근까, 간단하게 그대가 가진 GPU 램이랑 모형 크기를 비교할 거면 맥스 1 byte 또는 8Bit Quantized 모형을 감안하는 거고...

8B x 1 Byte = 8GB

안전하게 돌리려면 그보다 더 내려야 하는 거야. Half precision도 버거운 거라고. 4Bit Quantized 모형을 다운로드 받으면...

8B x 0.5 Byte = 4GB

남아도는 거지. 그래서 씽씽 돌릴 수 있고.

GPT4 모형이 1.7T 정도 파라미터라고 하는데, 이걸 Mixture of Experts 모형으로 여기면(이제 거의 그렇다고 알려져 있어) 8 x 220 = 1760B 정도 되는 거야. 

그래서 대충 GPT4가 8x220B MoE 모형이구나... 짱구들 굴리는 거고.

근데, 가끔 그대들도 느낄 거야 GPT4 dumb down 되는 거... 그래서 난 또 이런 짱구를 굴리는 거지. Quantized 또는 pruned 모형을 두고 스케일링하는 구나... 실제로 그럴 수밖에 없을 거야. 사용자가 많아지면. 

저 파라미터를 풀 정밀도로 돌리려면...

1.7T x 4 Bytes = 6.8TB

이걸 돌리려면 8TB GPU 램 클러스터가 필요하니까.

1.7T x 2 Bytes = 3.4TB

정밀도 반으로 낮춰도 이 정돈데... 대부분 사용자는 아주 기본적인 질문만 해대는데 견문발검할 이유 없으니까.

블랙스완 슥제야. 링크에 달린 영어 설명 포함해서 2번씩 읽고 혼자 썰 풀 수 있을 때까지 공부하셔.

🔗 The Official Domain for My Repo: https://cwkai.net
🔗 The Official Domain for My AI Artworks and Essays: https://creativeworksofknowledge.net
🔗 My Artstation Website: https://neobundy.artstation.com/