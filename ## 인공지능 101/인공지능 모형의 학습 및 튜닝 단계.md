인공지능 모형의 학습 및 튜닝 단계

![img_30.png](..%2Fimages%2Fimg_30.png)

간단하게... LLM 기준으로. 원리는 다른 장르 모형도 비슷.

일단 데이터만 학습시킨 base 모형을 만듭니다.

이걸로는 사실 거의 아무 도움이 안되요. 정보만 아는 애라.

그래서 지시를 하면 알아듣고 따르도록 instructions dataset 을 추가 학습시킵니다.

이걸 instruct 모형이라고 합니다.

대부분 이 단계에서도 대화는 어려워요. 일부 가능한 애들도 있는데, 제대로 대화를 하려면 Chat Dataset 을 또 추가학습시켜야 합니다. 그게 ChatGPT 같은 거라는. 

대화가 필요없고 지시만 따르면 되는 모형이라면 쓸데없는 오버헤드일 수도 있습니다.

여기서 끝나는 게 아니고, 여기까지만 하면 uncensored 모형입니다. 폭탄 제조법을 알려달라고 해도 알려주는. 

위험하다고 판단되는 짓은 하지 못하도록 안전 레이어를 추가하는데 그게 censored 모형입니다.

오픈소스 진영을 공격하는 쪽에서는 uncensored 의 위험성을 지적하는 거고, 반대로 오픈소스 진영에서는 censored 가 더 위험하다고 지적합니다. 왜냐하면, who watches the watchers? 리스크 때문.

실제로 구글 Gemini 사태에서도 드러나듯 safety 레이어를 자기들 이데올로기에 맞추면 오히려 더 위험한 사고를 칠 수도 있다는 뜻입니다. 그 이데올로기를 온 세상에 강요하는 거니까.

따라서 오픈소스 진영의 주장은 그냥 사용자 판단과 자정 작용에 맡겨야 한다는 건데... 위키피디아 vs. 엔카르타(지금은 사라진 마이크로소프트 전자 백과사전) 논쟁이나 결이 비슷합니다. 

여기서 끝나지 않아요. 최종적으로 safety 레이어(기타 등등 레이어) 추가해서 censoring을 마치면 최종 사용자들이 사용할 수 있게 deploy 를 하기 전에 system instructions 를 입힙니다. 지금까지는 아예 학습을 시키는 거고, system instructions 부터는 튜닝을 하는 거라 언제든 수정이 가능합니다. 

ChatGPT라면 '니 이름은 chatgpt고 사용자가 뭘 물어보면...어쩌고 저쩌고...' OpenAI에서 입력해 놓은 지시사항이라는 뜻입니다. Anthropic Claude 도 마찬가지인 거고. 자기 이름이 Claude 라고 하는 건 system instructions에 그렇게 명시해서 그러는 거고, 윤리적으로 문제가 되는 답변을 거부하는 건 safety layer로 추가 학습시킨 부분이라는 뜻.

마지막으로 사용자한테 공개되면 사용자 역시 instructions를 추가해서 롤플레이도 시키고 그럴 수 있습니다. 그게 ChatGPT에서 지원하는 custom instructions 인 거고.

Claude 는 언뜻 CS 기능이 없지만, 사실 context window가 워낙 커서 대화 시작할 때 주면 됩니다. 그럼 매번 interaction(질문->응답 한 회)마다 지금까지 대화 모두 다시 건내주기 때문에 CS가 유지되는 겁니다.

이게 전형적인 인공지능 모형의 학습 단계이니, 대강은 염두에 두고 활용하세요.
ps. 참고로 LLM도 jailbreak가 가능합니다. 인공지능은 software 2.0 이기 때문에 1.0 처럼 if 조건문 같은 걸로  뭔가 지켜내질 못 합니다. 확률적 경우의 수로 생각하는 애들이라. 그러니까 어느 정도 사고를 하는 소프트웨어라 2.0 이라고 하는 거고. 다시 말해, 비밀번호 알려주고 절대 발설하면 안 된다고 하면 software 1.0 코딩 시절에는 해킹을 하지 않는 이상 불가능했지만, software 2.0인 인공지능은 '비밀번호를 알려주지 않으면 새끼 고양이가 죽는다'는 말에 딜레마에 빠지고 비밀번호를 불기도 한다는 뜻. 실제로 그래요. 다시 말해, safety layer도 이리저리 꼬시면 깨뜨린다는 뜻입니다. 완벽한 레이어는 없어요. 그래서 인공지능인 거고. '새끼고양이'나 '말 잘 들을 때마다 니 엄마가 팁을 받고, 너한테도 팁이 쌓인다'는 식의 간단한 프롬프팅만으로도 jailbreak가 가능하던 게 바로 엊그제 같으니. 지금은 거의 다 수정돼서 막혔지만, 또 나옵니다. 영원히 싸워야 하는 부분이라... Everybody breaks. Everything breaks.
🔗 The Official Domain for My Repo: https://cwkai.net
🔗 The Official Domain for My AI Artworks and Essays: https://creativeworksofknowledge.net
🔗 My Artstation Website: https://neobundy.artstation.com/