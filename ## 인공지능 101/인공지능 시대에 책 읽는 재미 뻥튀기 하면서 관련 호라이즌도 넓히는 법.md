# 인공지능 시대에 책 읽는 재미 뻥튀기 하면서 관련 호라이즌도 넓히는 법

뭐 모범 답안이 있는 건 아니고... 

픽션이든 논픽션이든 상관 없어요. 어차피 글이라는 건 읽으면서 대리경험(vicarious experience)를 쌓아가는, 이 세상에서 직접 체험이 불가능한 부분의 샘플링 확장 매체이고, 필연적으로 상상력을 기르는 경험이니까 '글'이면 다 됩니다.

가령, 1Q84를 읽는다면 캐릭터 묘사가 대충은 나와요. 특히 주요 캐릭터면 더 그렇고. 일부러 외형을 얼버무리는 경우도 있는데, 그건 여러분 상상력으롬 매우라는 거고...

장면 묘사도 마찬가지. 

그럼 그걸 영어로 옮겨서 (이건 못하겠으면 GPT한테 부탁해도 되고) prompt 라는 걸 만들어요. 인물이든 장면이든... 

미드저니를 예로 들어볼게요. 오늘 올린 1Q84, 3권, 제5장을 상산한 겁니다. 

"/imagine prompt:  A cinematic shot of a beautiful young Japanese woman in her 20s with short straight hair, holding up a handgun with both hands beside her face, pressing her ear against a door while nervously listening to the sounds outside. Sweat beads on her forehead, highlighting her tension. The background shows a dimly lit room with shadows, creating a suspenseful atmosphere. The light source is soft, casting dramatic shadows on her face, with a focus on her determined expression. Created using: film noir style, shallow depth of field, 35mm film, chiaroscuro lighting, high contrast, cinematic composition, dynamic angles, tension-filled atmosphere --cref 캐릭터_이미지_업로드_링크"

이 정도면 살짝 오버한 건데, 아예 미드저니 프롬프트 만들어주는 커스텀 GPT도 있으니 그걸 이용하고 다듬어도 됩니다. 

미드저니만 쓰는 건 아니고, 레오나르도 같은 Stable Diffusion 기반 이미지 생성 AI 서비스도 있으니 그걸 써도 
됩니다.

품질은 (특히 시네마틱 리얼리즘) 아직 미드저니가 월등하지만 최대 단점은 ControlNet(베이스 이미지를 통한 장면 제어)가 어렵다는 겁니다. 그냥 베이스 이미지에 가중치(image weight: --iw 1.5 식) 주는 게 전부인데, 2가 최대치이고, 그마저도 제멋대로라 완전한 컨트롤이 불가능해요. 조만간 ControlNet 수준의 제어가 가능해진다고는 하는데, 아직 멀었을 겁니다. Stable Diffusion과는 덩치발이 다른 아키텍처라 (MOE 수준이 아닐까 의심되거든, 모형 여러개 묶어서 하나로 돌리는... 오픈소스가 아니라서 확실하지는 않지만, 대충 눈치로는 그래요. 참고로, 
GPT4도 MOE 랍니다. Mixture of Experts. 작은 모형을 묶어서 돌리는 거에요. 작다고는 해도 하나하나가 여느 오픈소스 대형 모형 수준이긴 합니다.).

Stable Diffusion은 ControlNet 이라는 메커니즘으로 거의 완벽한 제어가 가능해요. 물론, 무료라는 장점도 있고(로컬에 설치할 경우). 로컬에 설치하기 어렵거나 귀찮을 때 레오나르도 같은 SD 기반 서비스를 쓰는 겁니다. 표현
은 다른데 (ControlNet이란 말을 안 쓸 뿐) 방식은 똑같아요.

가장 좋은 건 둘을 섞는 겁니다. 그러다보면 자연스레 블렌딩도 하게 되고 뽀샵질도 익히게 되고... 그러면서 또 
이 바닥 호라이즌이 넓고 깊어져요.

시마네틱한 장면을 만드는 스킬도 늘고(cinematography)... 미드저니는 이런 장면을 워낙 많이 학습한 대형 모형
이라 cinematic 키워드만 들어가도 기본으로 그럴듯한 클리쉐 장면을 뽑아주거든요.

그러고 노는 겁니다. 그럼 책을 읽으면서 상상만 하던 게 두 눈으로 보이기 시작하는 건데...

문제는 캐릭터겠지요? 베이스 이미지로 캐릭터 만드는 걸 SD에서는 IP Adaptor 라고 합니다. 미드저니는 최근 들어서야 --cref 란 파라미터가 생겼는데 (위 예 참조) 참조시키고 싶은 캐릭터 이미지를 업로드 하고 그 URL을 적어주면 ("--cref URL"식, --cw 30 이면 캐릭터 가중치를 30% 참고...0이면 외형만 참고, 의상 등 다른 건 무시 등등) 그 캐릭터를 참조해서 비슷한 캐릭터를 일관되게 만들어 줍니다.

근데 이 워크플로우를 그대로 사용하면 베이스 이미지와 워낙 다른 캐릭터가 나오기 때문에 거의 못 쓸 수준
일 겁니다. 그래서 반대로 응용합니다. 거꾸로 그럴듯한 캐릭터가 잘 나오는 베이스 이미지를 찾는 겁니다. 

여러 차례 강조했지만, AI는 인간과 다른 방식으로 세상을 봅니다. 따라서, 인간 눈으로 보는 캐릭터와 AI가 해석하는 캐릭터 특징은 전혀 다를 수 있어요. 주로 학습 데이터 기반으로 edge detection을 통한 얼굴, 의상, 소품 특징만 잡아네는데 자주 쓰다보면 대충 감이 와요. 아, 원래 인간형(humanoid)만 되는 건데, 말이 그렇고 그냥 다른 것도 됩니다. 총, 칼, 몽키 렌치... 이딴 것도 되니까.

제가 만드는 1Q84 캐릭터들이 (대충은 일관된) 다 그렇게 만든 것.

따라서, 일관된 캐릭터(character consistency)가 잘 나오는 베이스 이미지를 모아두고 그때 그때 활용하면 된다는.

물론, AI 가 만들어낸 이미지만으로 만족할 수도 있고, 조금 더 욕심을 내서 포토샵 같은데서 필터도 입혀보고, 픽셀 유동화 도구(Liquify Tool) 써서 회형을 바꾸거나, 더 나아가 생성 이미지를 바탕으로 새로운 장면을 블렌딩
해서 그려내는 것도 가능해요. 저도 수십장 만들어서 하나하나 뜯어내고 블렌딩하는 장면도 많아요.

오늘 아오메마가 들고 있는 총 같은 것도 너무 크게 나오면 안 되는데 크게 나왔다 싶으면 inpaint (AI를 이용한 부분 수정) 기능을 쓰기보다 그냥 블렌딩해서 다시 그리는 게 나을 수도 있어요. 얼굴도 수정한 거고, 눈물 자국
도 수정한 겁니다. 

그림까지 그릴 줄 알면 더 쉽고 재밌어 집니다.

픽션 뿐 아니라, 논픽션도 가능하고...

대충 훑어좌고 재미도 있고, 호라이즌도 상당히 넓고 깊어지지 않을까... 그런 생각 안 드시나?

아니면 말고... 🤗
