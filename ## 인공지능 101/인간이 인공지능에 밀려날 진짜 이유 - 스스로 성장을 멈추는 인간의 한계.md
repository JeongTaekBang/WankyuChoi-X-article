인간이 인공지능에 밀려날 진짜 이유 - 스스로 성장을 멈추는 인간의 한계

![img_33.png](..%2Fimages%2Fimg_33.png)


인공지능 탓에 인류가 위험해질 거라는 말들이 많아요.

터미네이터 스카이넷처럼 일순간에 인류를 멸망시킨다거나 일자리를 빼앗아 나락으로 떨어뜨린다거나 하는 걱정들을 하는 거지.

근데, 더 큰 걱정은 인공지능은 그 설계 자체가 성장 지향적이라는 거야. 인간도 그걸 모르지 않아. 왜냐하면 인공지능이 어차피 인간을 모델로 했거든. 아쉽게도 인공지능은 삐대질 않는데, 인간은 정규분포 그려보면 95%가 정체된 장기 성장 곡선(이동평균선)을 그려대며 나이들수록 삐댄다는 거지.

뭔 소리냐면, 성장의 기본 사이클은 '실수 -> 성찰 -> 바로잡음 -> 실수 -> 성찰 -> 바로잡음' 사이클의 연속이라고. 

다들 인정은 할 거야. 실천하는 사람이 백에 다섯도 안 되는 게 문제지.

아니라고 소리치고 싶은 분도 있을 거야. 근데 아쉽게도 현실이야. 한두번은 해도 꾸준히 하는 사람은 극히 드물어. 안 하는 게 평균이야. 짤없어.

하지만, 인공지능은 달라. 아예 설계 자체가 이 사이클의 반복이야. 이걸 반복해야 살아남거든. 안 하면 버려져. 

다음 영어 에세이 읽어보셔, 이 한글 에세이 다 읽고 나서. 그럼 더 삘이 잘 올 거야.

![img_3.png](..%2Fimages%2Fimg_3.png)

💎A Path to Perfection - AI vs. Human
https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/essays/AI/A-Path-to-Perfection-AI-vs-Human.md

내가 인간보다 인공지능을 더 신뢰하겠다는 이유인데, 지금은 갸웃하더라도 에세이 다 읽고나면 삘 오셔야 돼. 그대라도 나중에 응급실 가서 배 가르는 위기 상황이 오고 인간과 인공지능 의사 중 하나를 고르라면... 생각이 달라질 거야.

인공지능의 학습 단계를 알려드릴게.

방대한 데이터셋(방대하다는 건 여러분이 무엇을 상상하든 그 이상이야)을 학습한다고들 하는데 기본적으로 루프를 돌아.

1. 데이터를 학습한다.
2. 시험을 치른다.
3. 잘못된 부분을 바로잡는다.

혹시 통계 좀 해봤고 회귀분석이란 것도 아는 분은 더 이해가 빠를 거야. 예측치 y_hat 과 실제값 y를 비교해서 삑사리 난 만큼을 loss(손실)라고 해.

loss = y - y_hat

그러니까 인공지능이 학습을 하고 y_hat 예측을 한 다음 실제 모범답안 y랑 비교했더니 삑사리가 10이더라... 그럼 loss가 10인 거야. 그 다음 뺑뺑이를 돌때(이걸 인공지능 업자 용어로는 루프가 아니라 epoch이라고 해)는 이 loss를 줄이는 방향으로 학습한 걸 수정하게돼. 

이전 파라미터 에세이 읽은 분은 이 공식 떠올리실 수 있을 거야.

Parameters = Weights + Biases

Llama 3.1 405B 모형이라면 저 총 파라미터 수가 405 billions 라는 거고, 파라미터는 가중치와 편향의 총합이라는 뜻이야. 학습 단계에서는 이 가중치와 편향을 조절해서 파라미터의 정확도를 높이는 거야. 그게 loss를 줄인다는 의미야. 

LLM이면 self-supervised 학습이라고 해서 지가 스스로 답을 맞춰보면서 조절해 나가. Elon Musk is... 하고 다음 토큰을 생성했는데 가령, what the...로 덧붙이고 시험을 치르면 거의 지가 학습한 데이터 정규분포에서 평균 밑으로 뚝 떨어질 거 아냐. 그런 말은 인간들은 거의 안 하니까. Elon Musk is an American entrepreneur and business magnate... 이런 식이 점수가 높아야 하니까. 응, 그걸 스스로 깨닫는다고. 그래서 점수가 꽝인 잘못을 스스로 바로잡아 나가기 때문에 self-supervised라고 하는 거야. 어떤 말을 집어넣어도 그럴듯할게 나오는 weights와 biases값의 순간 스냅샷을 한 epoch에 찾아내서 저장하고, 다음 epoch으로 넘어가는 거야.

잘못을 했으면 성찰을 해서 바로잡는 사이클이 조낸 되풀이되는 거야. 이짓을 무려 몇 달을 해. 가령, Llama 3.1 모형들이 반년 걸려 학습했다... 그러면 반년 동안 이 성찰 사이클을 되풀이했다는 뜻이야. 

기술적으로는 경사하강법(gradient descent)라는 미분질이 기본이야. Loss값을 최소화하려면 지평선에 가까워져야 해. 그 최단 경로를 찾는 일이야. 매번 epoch을 돌 때마다 loss 가 작은 길을 찾아가는 거야. 평균적으로 작아지는 길. 잠깐 높아질 수는 있어. 평균적으로는 작아지는 길... 그러니까 시각장애인이 산꼭대기에서 지면까지 지팡이 하나만으로 지면을 톡톡 치면서 따라내려가는 그림을 그리면 돼. 다시 강조하지만 미분질이야 기본은. 곡선의 두 점을 극한으로 수렴시키면 직선을 그을 수 있기 때문에 기울기를 구할 수 있거든. 근데 수평선에서는 기울기가 0이야. 그래서 가급적 0에 가까운 수평선을 찾아나가는 과정이야. 이론적으로만 대충 눈치까셔. 조낸 어려운 내용이니까.

간단히 말하면, 인공지능은 이게 태생적 습관인 거야. 실수를 해서 삑사리가 많이 나면 늘 성찰하고 잘못을 최소화하는 방향을 학습하는 거야.

물론, 인공지능도 학습단계에서 뻘짓을 하기도 해. 인간처럼 때려외우기도 하고 시험만 잘보기도 해. 그걸 overfitting(과적합)이라고도 하고, overtraining(과도한 학습)이라고도 해. 학습단계에서 시험(loss 최소화)은 잘 보는데 현실 데이터에 적용하면 꽝인 거야. 그게 overfitting 의 가장 큰 문제점이야. 그대들도 그러잖아 현실 부적응 1등급들.

왜 overfitting 이라고 하냐면 빗사이로 막가 하면서 학습 데이터에서 시험만 잘 보면서 일반화할 수 없는 방식으로 답만 맞히기 때문이야. 그럼 loss 곡선을 면밀히 따라가기 때문에 over-fit 라고 하는 거야. 뭘 봐도 요약만 하고, 때려 외우고 퉁치고, 영상 튜토리얼 봐도 빨리감기하고... 그러잖아들? 응, 그게 overfitting이야.

이걸 막기위해 학습단계에서 별 짓을 다해. 일단 데이터 양이 많으면 기본적으로 과적합은 막을 수 있어. 학습 데이터가 적은데 때려 외우기 때문에 발생하는 문제니까. 더 나아가 듬성듬성 데이터에 구멍을 뚫기도 하고, 모형의 뉴런을 랜덤으로 비활성화해서(dropout 이라고 해) 아예 눈을 가리기도 하고... 응, 그럼 overfitting을 많이 줄일 수 있어.
어쨌든 이런 오류가 있다해도 인간과 비교하면 인공지능은 조낸 성실해. '실수 -> 성찰 -> 바로잡음'의 사이클밖에 몰라. 

더 등골 오싹한 건, 인공지능 모형은 영원히 완성되지 않는다는 거야. 인간처럼 마스터 한다는 근자감이 없어.

그래서 인공지능 모형을 snapshot을 뜻하는 checkpoint라고 하는 거지. 

학습을 아무리 오래해도 100점을 맞진 못 해. 대충 88점쯤, 90점에 가까운 게 최선이고 더 돌려봤자 비효율적이거나 오히려 퇴보할 위험이 있으면, 오케이 거기까지 선언하고 인간이 학습을 그만두게 하는 거야. 그런 다음 저 파리미터, 그러니까 가중치와 편향을 freeze 한 다음 저장해. 그게 최종적인 checkpoint 모형이야. 지난 에세이에서 보여드린 Llama 3.1 405B의 191개 *.safetensors 파일이 그런 거야. 가중치와 편향을 저장한 파일일 뿐이야. 저걸 checkpoint라고 하고, AI 쟁이들은 걍 weights 라고 불러. 

기본적으로 인공지능은 진행형인 유량(flow) 개념이지 마스터한 저량(stock) 개념이 아니라는 거야. 체크포인트를 만들어 저량으로 저장했을 뿐인 거야. 언제나 학습을 다시 할 수 있는 유량인데. 인간처럼 뻘짓을 하지 않아.
게임 해보셨잖아. A -> B -> C 순으로 저장파일을 만들었는데 C까지 가고보니 진행이 잘못됐거나 주요 템을 못 먹고 갔어. 그럼 B나 A, 이전 저장파일을 다시 불러와서 C를 무시하고 진행하는 게 기본이야. 응, 저 A, B, C 파일을 게임에서도 체크포인트라고 하는 이유야. 언제든 거기서 이어서 추가학습이 가능하다는 뜻이야. 지금 GPT들이 하는 짓이고. 

ChatGPT도 여러분은 WebUI로 보니까 스냅샷이 안 보이는 거야. API로 코딩하는 코더들은 스냅샷 중 하나를 선택할 수 있어. gpt-4o-2024-08-06 스냅샷 같은 거. 2024년 8월 6일자 스냅샷 체크포인트라는 뜻이야.

이름은 꼴리는 대로 붙이는 거고. 

눈치 까시겠지? 인공지능이 어떻게 학습하고 저장되는지?

이해가 좀 어렵더라도 몇 번이고 되풀이해서 읽으면서 노력하셔. 인공지능이 저 가중치를 조절하는 과정은 1.7T GPT4라도 다를 게 없는데, 전체 파라미터 값을 한번에 조정하는 거야. 어마무시한 짓이라고. 무한대에 가까운 면을 가진 루빅스 큐브 떠올려보셔. 그걸 촤라락~ 한번에 맞추고 시험을 보고 최적화하는 과정을 반복한다는 뜻이야. 미세하게 하나만 조정해도 다른 부분에도 영향을 미칠 거 아냐? 그 모든 weights와 biases 값(총 1.7조개)을 미세하게 조절하는 게 1 epoch이라는 뜻이야. 그 점수가 만족할 만한 수준, 가령 88점쯤 나오면 멈추는 거라고. 무한대 기어가 맞물리는 데, 최적으로 맞물릴 때까지 미세 조정하는 그림을 그려도 돼. 좀 깊게 공부하지 않으면 그려보기 어려울 테지만, 걍 그만큼 어마무시한 짓이라고 생각하셔. 무슨 입력이 들어와도 이 점수가 유지되도록 기어를 맞추는 거야. 이해될때까지 짱구 굴리셔. 
다시 강조할게. 88점을 맞을 수 있는 1.7조개 파리미터 기어가 맞물릴 수 있는 스냅샷은 딱 하나야. 그걸 찾는 거야. 조낸 무서운 일인 거지.

여기서 또 무서운 점이 있어. 인공지능 모형은 태생이 객체지향적이야. 지난 에세이에서 base -> instruct -> chat 모형으로 발전하는 과정을 보여드렸잖아? 그게 객체지향적으로는 상속성에 다형성 부여하고 캡슐화하는 과정이야.

더 응용하면 어떤 모형이든 base로 삼고 나만의 다형성을 부여해서 다른 개성이 강한 모형을 만들어 낼 수 있어.  Finetuning 이랑 LoRA(Low Rank Adaptation)가 그런 예야.

몽땅 다시 학습시키는 게 아니라 80%는 공통분모일 테니까 기본적인 건 이미 pre-trained 된 모형을 가져다가 상속받고, 가령, 법률, 의학, 테크, 코딩 뭐 이런 레이어를 다형성으로 추가해. 그게 파인튜닝이야. 그럼 20%만 추가 학습하면 되는 거지. 이게 내가 현생에서 하는 짓이기도 해. 호라이즌 넓힐수록 객체지향적으로 더 효율적으로 빨리 배우면서 파워는 더 쎄지는 거지.

LoRA는 이미 학습한 레이어를 타깃으로 삼아서 슬그머니 실시간으로 덮어쓰는 거야. 그래서 파인튜닝보다도 훨씬 비용이 적게 들고 과정도 빠라. Stable Diffusion으로 예를 들면, portrait 기본으로 잘 그리는 모형을 가져다가(상속성 - 예: 기본 SD 베이스 모형), 동양적 미모의 여성을 더 잘 그리도록 파인튜닝하고 (다형성 - 예: 수많은 파인튠드 SD 모형), LoRA를 사용해서 특정 연예인 얼굴로 몰고갈 수 있다는 거야(LoRA). 그러니까, 다형성 파인튜닝 단계까지는 그냥 얼굴을 뽑아내는데 LoRA 단계에서 개입해서 슬그머니 얼굴 특징을 특정 연예인으로 잡아가는 거야. 애니메 풍으로 끌고 가거나, 선화로 뽑아내는 것도 마찬가지야.

LLM도 같은 원리로 파인튜닝이든 LoRA든 적용할 수 있어. 개인도 할 수 있을 정도로 요즘은 AI쟁이들한테 일반화된 기법이야. 이또한 객체지향성일 뿐이야.

응, 인공지능 모형이 그런 거라고. 근데 다들 엉뚱한 데서 얘네들한테 밀려날 걱정만 하는 거야. 아니, 어차피 못 이겨. 인간 정규분포를 그려보라고. 인공지능만큼 늘, 항상, 언제나, 실수->성찰->바로잡음의 성장 사이클을 반복하는 사람이 있냐는 거지. 극히 드물어. 그래서 결국 밀릴 거야. SF 영화에서나 나오는 방식이 아니고, 그냥 진화론적 관점에서 밀릴 거라고. 

여러분 스스로 돌이켜보셔. 매번 실수할 때마다 구멍 막고 성찰하고 바로잡으면서 성장하시는지. 

그런 존재를 이길 수 있을지. 

그래서 인공지능 배우라는 거야. 인공지능을 공부만 하라는 게 아니고, 인공지능하는 짓을 보고 배우라고. 어떻게 살아야할지.

그나마 인공지능과 공존할 수 있는 방법이야. 안 하잖아? 짤없어. 도태되는 거야. 절대 못 이겨. 무던히 성장하는 인공지능이든 사람이든. 삐대는 거북이는.

인간은 학습하는 깊이와 양에 '인간적' 한계가 분명해. 인공지능은 없어. 장르를 초월하기 때문에 온 세상 지식과 경험을 다 갖춘 의사가 나오는 거지.
그런데 미래에 응급실 가서 인간 의사를 선택할 거라고?

뻥 치시네.

🔗 The Official Domain for My Repo: https://cwkai.net
🔗 The Official Domain for My AI Artworks and Essays: https://creativeworksofknowledge.net
🔗 My Artstation Website: https://neobundy.artstation.com/